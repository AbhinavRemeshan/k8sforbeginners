# Kubernetes Troubleshooting Guide

## Introduction

Kubernetes, while immensely powerful, can present a variety of challenges during day-to-day operations. From application deployment issues to cluster-level failures, having a structured approach to diagnosing and resolving these problems is essential. This troubleshooting guide aims to assist users in identifying common issues encountered in Kubernetes environments and provides actionable steps to resolve them effectively.

By following this guide, you will gain insights into:
- Diagnosing pod-related issues.
- Investigating cluster health and node failures.
- Resolving networking and service connectivity problems.
- Debugging persistent storage and volume-related errors.
- Troubleshooting authentication and role-based access control (RBAC).

This guide builds upon practical lab exercises to ensure a hands-on understanding of Kubernetes troubleshooting. Use it as a reference for identifying and resolving issues in real-world scenarios. The structure of this document will evolve as we incorporate detailed steps and case studies from provided lab materials.

# Kubernetes Troubleshooting Areas

## Section 1: Troubleshooting Kubernetes Cluster

### Areas to Troubleshoot

- Cluster health and configuration.
- Node statuses and connectivity.
- Namespace-specific issues.

### Possible Scenarios

- The cluster is unresponsive or partially operational.
- Nodes are not in a ready state.
- Namespace-specific issues causing resource unavailability.

### Steps and Commands to Troubleshoot

#### 1. View Nodes in the Cluster

**Command:**

```bash
kubectl get nodes
```

Provides an overview of all nodes and their statuses.

#### 2. Fetch Cluster Information

**Command:**

```bash
kubectl cluster-info
```

Details about the cluster's control plane and components.

#### 3. Obtain a Cluster Dump

**Command:**

```bash
kubectl cluster-info dump
```

Generates diagnostic information about the cluster's state.

#### 4. Explore Dump Command Options

**Command:**

```bash
kubectl cluster-info dump --help
```

Displays available options for generating a cluster dump.

#### 5. Generate Namespace-Specific Dump

**Command:**

```bash
kubectl cluster-info -n <namespace> dump
```

For example, to diagnose issues in the `test` namespace:

```bash
kubectl cluster-info -n test dump
```

#### 6. Check Cluster Component Health

**Command:**

```bash
kubectl get componentstatus
```

Reports the health of cluster components such as etcd, scheduler, and controller-manager.

### Things to Check

- Are all nodes in a `Ready` state?
- Is the control plane accessible using `kubectl cluster-info`?
- Are specific namespaces experiencing issues?
- Are cluster components reporting healthy statuses?

By following these steps, you can effectively diagnose and resolve common Kubernetes cluster issues.

---

## Section 2: Understanding Kubernetes Logging Architecture

### Areas to Troubleshoot

- Application logs and debugging.
- Container-level logging.
- Namespace-wide log aggregation.

### Possible Scenarios

- Logs are not accessible for a specific pod or container.
- Need to debug application behavior using logs.
- Cluster-wide log monitoring requirements.

### Steps and Commands to Troubleshoot

#### Step 1: Get Help with Logging

**Command:**

```bash
kubectl logs --help
```

Displays available options and flags for fetching logs.

#### Step 2: Create a Pod and View Logs

1. Create a YAML file for the pod: **Command:**

   ```bash
   vi busybox.yaml
   ```

   Add the following content to the file:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: counter
   spec:
     containers:
     - name: count
       image: busybox:1.28
       args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
   ```

2. Deploy the pod and fetch logs: **Command:**

   ```bash
   kubectl apply -f busybox.yaml
   kubectl logs counter
   ```

3. Retrieve the last five lines of logs: **Command:**

   ```bash
   kubectl logs counter --tail=5
   ```

#### Step 3: Advanced Log Options

1. Fetch logs from all containers in a namespace: **Command:**

   ```bash
   kubectl logs counter --all-containers
   ```

2. Fetch logs from a specific time range: **Command:**

   ```bash
   kubectl logs counter --since=<timespan>
   ```

   Example: To retrieve logs from the last hour:

   ```bash
   kubectl logs counter --since=1h
   ```

### Things to Check

- Are logs being generated by the pod/container?
- Is the `kubectl logs` command correctly targeting the pod or namespace?
- Are appropriate log filters, such as `--tail` or `--since`, being applied?

### Tools and Prerequisites

- Tools: `kubectl`, `kubeadm`, `kubelet`, `containerd`.
- Prerequisite: A functioning Kubernetes cluster.

By following these steps, you can effectively monitor and troubleshoot application logs in Kubernetes, ensuring better visibility into system behavior and application performance.

---

## Section 3: Understanding Cluster and Node Logs

### Areas to Troubleshoot

- Control-plane component logs.
- Worker node logs.
- Logs from services like the API server, controller manager, etcd, and kubelet.

### Possible Scenarios

- Control-plane components are unresponsive or showing errors.
- Worker nodes are not functioning as expected.
- Logs are inaccessible or incomplete.

### Steps and Commands to Troubleshoot

#### Step 1: View Control-Plane Component Logs
1. Navigate to the logs directory:
   **Command:**
   ```bash
   cd /var/log/pods
   ls
   ```
   Lists all the components and their directories.

2. Navigate to the API server logs:
   **Command:**
   ```bash
   cd kube-system_kube-apiserver-<master-node>_<unique-id>/kube-apiserver
   ```

3. View the latest log file:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 2: View Controller Manager Logs
1. Navigate to the controller manager logs directory:
   **Command:**
   ```bash
   cd kube-system_kube-controller-manager-<master-node>_<unique-id>/kube-controller-manager
   ```

2. View the logs:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 3: View etcd Logs
1. Navigate to the etcd logs directory:
   **Command:**
   ```bash
   cd kube-system_etcd-<master-node>_<unique-id>/etcd
   ```

2. View the logs:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 4: View Worker Node Logs
1. View the kubelet service logs:
   **Command:**
   ```bash
   sudo journalctl -xu kubelet -n
   ```
   Press `q` to exit the log viewer.

2. View pod logs on the worker node:
   **Command:**
   ```bash
   cd /var/log/pods/
   ls
   ```
   Navigate into specific pod directories to view logs.

### Things to Check

- Are control-plane component logs accessible and free of errors?
- Are kubelet service logs showing any issues on worker nodes?
- Is the logging directory structure consistent across nodes?

### Tools and Prerequisites

- Tools: `kubeadm`, `kubectl`, `kubelet`, `containerd`.
- Prerequisite: A functioning Kubernetes cluster.

By following these steps, you can inspect and troubleshoot control-plane and worker node logs, ensuring smooth cluster operations.

---

## Section 4: Troubleshooting Node Readiness

### Areas to Troubleshoot

- Node readiness and status.
- Kubelet service issues on worker nodes.
- Node transitioning between `Not Ready` and `Ready`.

### Possible Scenarios

- Worker node status shows `Not Ready`.
- Kubelet service on a worker node is inactive or malfunctioning.
- Issues after restarting or disabling a worker node.

### Steps and Commands to Troubleshoot

#### Step 1: Check the Node Status on the Master Node
1. View the status of all nodes in the cluster:
   **Command:**
   ```bash
   kubectl get nodes
   ```

2. Identify the problematic worker node (e.g., `worker-node-2`) from the output.

#### Step 2: Disable the Worker Node and Diagnose
1. Stop the kubelet service on the worker node:
   **Command:**
   ```bash
   sudo service kubelet stop
   ```

2. Verify the kubelet service status:
   **Command:**
   ```bash
   sudo service kubelet status
   ```
   (Press `q` to exit the status viewer.)

3. Check the node's status from the master node after stopping the kubelet service:
   **Command:**
   ```bash
   kubectl get nodes
   ```
   The node status should show `Not Ready`.

4. Describe the node to diagnose the problem:
   **Command:**
   ```bash
   kubectl describe node worker-node-2.example.com
   ```

#### Step 3: Fix the Worker Node
1. Start the kubelet service on the worker node:
   **Command:**
   ```bash
   sudo systemctl start kubelet
   ```

2. Verify the kubelet service status:
   **Command:**
   ```bash
   sudo systemctl status kubelet
   ```
   (Press `q` to exit the status viewer.)

3. After a few minutes, recheck the node's status from the master node:
   **Command:**
   ```bash
   kubectl get nodes
   ```
   The node status should now display `Ready`.

### Things to Check

- Is the kubelet service running correctly on the worker node?
- Are there any errors in the output of `kubectl describe node`?
- Does the node transition back to `Ready` after restarting the kubelet service?

### Tools and Prerequisites

- Tools: `kubeadm`, `kubectl`, `kubelet`, `containerd`.
- Prerequisite: A functioning Kubernetes cluster.

By following these steps, you can successfully diagnose and resolve issues causing a worker node to transition from `Not Ready` to `Ready`.

---

Sure! Here's Section 5, formatted for you to append to the original guide:

---

### Section 5: Understanding Container Logs

In this section, you will learn how to check and access container logs using `crictl` commands. These logs are crucial for troubleshooting and understanding container behavior.

#### Steps to Check Container Logs:

1. **Navigate to Worker Node:**
   - Log in to the worker-node-2 in the LMS dashboard.

2. **Fetch the Container ID:**
   - Use the following command to fetch the container ID:
     ```bash
     sudo crictl ps -a
     ```

3. **Access and View Container Logs:**
   - To view the logs of a specific container, use the following command:
     ```bash
     sudo crictl logs <container_id>
     ```
     Replace `<container_id>` with the actual container ID from the previous command.

4. **Retrieve the Latest Log Entry:**
   - If you need to retrieve the most recent log entry, use the following command:
     ```bash
     sudo crictl logs --tail=1 <container_id>
     ```
     This will return the latest log entry for the specified container.

By following these steps, you have successfully demonstrated how to use `crictl` commands to monitor container logs and troubleshoot container-related issues.

---




