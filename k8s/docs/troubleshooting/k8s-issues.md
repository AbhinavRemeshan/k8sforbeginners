# Kubernetes Troubleshooting Guide

## Introduction

Kubernetes, while immensely powerful, can present a variety of challenges during day-to-day operations. From application deployment issues to cluster-level failures, having a structured approach to diagnosing and resolving these problems is essential. This troubleshooting guide aims to assist users in identifying common issues encountered in Kubernetes environments and provides actionable steps to resolve them effectively.

By following this guide, you will gain insights into:
- Diagnosing pod-related issues.
- Investigating cluster health and node failures.
- Resolving networking and service connectivity problems.
- Debugging persistent storage and volume-related errors.
- Troubleshooting authentication and role-based access control (RBAC).

This guide builds upon practical lab exercises to ensure a hands-on understanding of Kubernetes troubleshooting. Use it as a reference for identifying and resolving issues in real-world scenarios. The structure of this document will evolve as we incorporate detailed steps and case studies from provided lab materials.

# Kubernetes Troubleshooting Areas

## Section 1: Troubleshooting Kubernetes Cluster

### Areas to Troubleshoot

- Cluster health and configuration.
- Node statuses and connectivity.
- Namespace-specific issues.

### Possible Scenarios

- The cluster is unresponsive or partially operational.
- Nodes are not in a ready state.
- Namespace-specific issues causing resource unavailability.

### Steps and Commands to Troubleshoot

#### 1. View Nodes in the Cluster

**Command:**

```bash
kubectl get nodes
```

Provides an overview of all nodes and their statuses.

#### 2. Fetch Cluster Information

**Command:**

```bash
kubectl cluster-info
```

Details about the cluster's control plane and components.

#### 3. Obtain a Cluster Dump

**Command:**

```bash
kubectl cluster-info dump
```

Generates diagnostic information about the cluster's state.

#### 4. Explore Dump Command Options

**Command:**

```bash
kubectl cluster-info dump --help
```

Displays available options for generating a cluster dump.

#### 5. Generate Namespace-Specific Dump

**Command:**

```bash
kubectl cluster-info -n <namespace> dump
```

For example, to diagnose issues in the `test` namespace:

```bash
kubectl cluster-info -n test dump
```

#### 6. Check Cluster Component Health

**Command:**

```bash
kubectl get componentstatus
```

Reports the health of cluster components such as etcd, scheduler, and controller-manager.

### Things to Check

- Are all nodes in a `Ready` state?
- Is the control plane accessible using `kubectl cluster-info`?
- Are specific namespaces experiencing issues?
- Are cluster components reporting healthy statuses?

By following these steps, you can effectively diagnose and resolve common Kubernetes cluster issues.

---

## Section 2: Understanding Kubernetes Logging Architecture

### Areas to Troubleshoot

- Application logs and debugging.
- Container-level logging.
- Namespace-wide log aggregation.

### Possible Scenarios

- Logs are not accessible for a specific pod or container.
- Need to debug application behavior using logs.
- Cluster-wide log monitoring requirements.

### Steps and Commands to Troubleshoot

#### Step 1: Get Help with Logging

**Command:**

```bash
kubectl logs --help
```

Displays available options and flags for fetching logs.

#### Step 2: Create a Pod and View Logs

1. Create a YAML file for the pod: **Command:**

   ```bash
   vi busybox.yaml
   ```

   Add the following content to the file:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: counter
   spec:
     containers:
     - name: count
       image: busybox:1.28
       args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
   ```

2. Deploy the pod and fetch logs: **Command:**

   ```bash
   kubectl apply -f busybox.yaml
   kubectl logs counter
   ```

3. Retrieve the last five lines of logs: **Command:**

   ```bash
   kubectl logs counter --tail=5
   ```

#### Step 3: Advanced Log Options

1. Fetch logs from all containers in a namespace: **Command:**

   ```bash
   kubectl logs counter --all-containers
   ```

2. Fetch logs from a specific time range: **Command:**

   ```bash
   kubectl logs counter --since=<timespan>
   ```

   Example: To retrieve logs from the last hour:

   ```bash
   kubectl logs counter --since=1h
   ```

### Things to Check

- Are logs being generated by the pod/container?
- Is the `kubectl logs` command correctly targeting the pod or namespace?
- Are appropriate log filters, such as `--tail` or `--since`, being applied?

### Tools and Prerequisites

- Tools: `kubectl`, `kubeadm`, `kubelet`, `containerd`.
- Prerequisite: A functioning Kubernetes cluster.

By following these steps, you can effectively monitor and troubleshoot application logs in Kubernetes, ensuring better visibility into system behavior and application performance.

---

## Section 3: Understanding Cluster and Node Logs

### Areas to Troubleshoot

- Control-plane component logs.
- Worker node logs.
- Logs from services like the API server, controller manager, etcd, and kubelet.

### Possible Scenarios

- Control-plane components are unresponsive or showing errors.
- Worker nodes are not functioning as expected.
- Logs are inaccessible or incomplete.

### Steps and Commands to Troubleshoot

#### Step 1: View Control-Plane Component Logs
1. Navigate to the logs directory:
   **Command:**
   ```bash
   cd /var/log/pods
   ls
   ```
   Lists all the components and their directories.

2. Navigate to the API server logs:
   **Command:**
   ```bash
   cd kube-system_kube-apiserver-<master-node>_<unique-id>/kube-apiserver
   ```

3. View the latest log file:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 2: View Controller Manager Logs
1. Navigate to the controller manager logs directory:
   **Command:**
   ```bash
   cd kube-system_kube-controller-manager-<master-node>_<unique-id>/kube-controller-manager
   ```

2. View the logs:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 3: View etcd Logs
1. Navigate to the etcd logs directory:
   **Command:**
   ```bash
   cd kube-system_etcd-<master-node>_<unique-id>/etcd
   ```

2. View the logs:
   **Command:**
   ```bash
   ls -la
   sudo cat 0.log
   ```

#### Step 4: View Worker Node Logs
1. View the kubelet service logs:
   **Command:**
   ```bash
   sudo journalctl -xu kubelet -n
   ```
   Press `q` to exit the log viewer.

2. View pod logs on the worker node:
   **Command:**
   ```bash
   cd /var/log/pods/
   ls
   ```
   Navigate into specific pod directories to view logs.

### Things to Check

- Are control-plane component logs accessible and free of errors?
- Are kubelet service logs showing any issues on worker nodes?
- Is the logging directory structure consistent across nodes?

### Tools and Prerequisites

- Tools: `kubeadm`, `kubectl`, `kubelet`, `containerd`.
- Prerequisite: A functioning Kubernetes cluster.

By following these steps, you can inspect and troubleshoot control-plane and worker node logs, ensuring smooth cluster operations.





